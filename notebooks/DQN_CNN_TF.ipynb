{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import gym_2048\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = False\n",
    "\n",
    "if logger:\n",
    "    log_dir = 'runs/2048_run_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    import os, shutil\n",
    "\n",
    "    if os.path.exists(log_dir):\n",
    "        shutil.rmtree(log_dir)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    if logger: writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN_CNN, self).__init__()\n",
    "        self.conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(4,4,1))\n",
    "        self.conv2 = Conv2D(64, (3, 3), activation='relu')\n",
    "        self.pool = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.dense2 = Dense(4, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, board_size):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features=board_size, out_features=128)\n",
    "#         # self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "#         self.out = nn.Linear(in_features=128, out_features=4)  # 4 actions\n",
    "\n",
    "#     def forward(self, t):\n",
    "#         t = t.flatten(start_dim=1)\n",
    "#         t = F.relu(self.fc1(t))\n",
    "#         # t = F.relu(self.fc2(t))\n",
    "#         t = self.out(t)\n",
    "#         return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        self.exploration_rate = None\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        self.exploration_rate = self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy: EpsilonGreedyStrategy, num_actions):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.explore_count = 0\n",
    "        self.exploit_count = 0\n",
    "\n",
    "    def print_explore_exploit_ratio(self):\n",
    "        print(\"Explore:\", self.explore_count / self.current_step, self.explore_count)\n",
    "        print(\"Exploit:\", self.exploit_count / self.current_step, self.exploit_count)\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            self.explore_count += 1\n",
    "            # self.print_explore_exploit_ratio()\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return tf.Tensor([action])\n",
    "        else:\n",
    "            self.exploit_count += 1\n",
    "            # self.print_explore_exploit_ratio()\n",
    "#             with torch.no_grad():\n",
    "                # print(\"================EXPLOIT================\")\n",
    "                # print(state)\n",
    "                # print(state.shape)\n",
    "                # print(policy_net(state))\n",
    "                # print(policy_net(state).argmax(dim=1))\n",
    "                # exit(0)\n",
    "            return tf.stop_gradient(policy_net(state).argmax(dim=1), name=None)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    numerator = tf.math.log(x)\n",
    "    denominator = tf.math.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym2048EnvManager():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('2048-v0')\n",
    "        self.env.reset()\n",
    "        self.current_state = None\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_state = None\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def num_action(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "    def take_action(self, action):\n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return self.normalize_state_reward(tf.Tensor([reward]).float())\n",
    "\n",
    "    def normalize_state_reward(self, val):\n",
    "        zero_pos = (val == 0)\n",
    "        val = torch.log2(val) / 15\n",
    "        val[zero_pos] = 0\n",
    "        return val\n",
    "\n",
    "    def get_max_tile(self):\n",
    "        return np.max(np.array(self.env.board))\n",
    "\n",
    "    def get_board_size(self):\n",
    "        return self.env.size ** 2\n",
    "\n",
    "    def get_state(self):\n",
    "        self.current_state = self.env.board\n",
    "        state = np.array(self.current_state)\n",
    "        state = state.reshape(-1)  # / 4096\n",
    "        state = tf.convert_to_tensor(state).float().unsqueeze(0)\n",
    "        state = self.normalize_state_reward(state)\n",
    "        if self.done:\n",
    "            state = torch.zeros_like(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "\n",
    "\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Max Tile')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    # print(\"Episode\", len(values), \"\\n\", \\\n",
    "    #       moving_avg_period, \"episode moving avg:\", moving_avg[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "gamma = 0.9955\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.0001\n",
    "target_update = 25\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "em = Gym2048EnvManager()\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_action())\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN_CNN()\n",
    "target_net = DQN_CNN()\n",
    "\n",
    "target_net.set_weights(policy_net.get_weights())\n",
    "\n",
    "maximum_tiles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    return t1, t2, t3, t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next_actions(policy_net, next_states):\n",
    "        next_actions = policy_net(next_states).argmax(axis=1).unsqueeze(-1).detach()\n",
    "        return next_actions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next_state_q_values(target_net, next_states):\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        next_state_q_values = torch.zeros(batch_size, 4).to(QValues.device)\n",
    "        next_state_q_values[non_final_state_locations] = target_net(non_final_states).detach()\n",
    "        return next_state_q_values\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next_double_dqn(policy_net, target_net, next_states):\n",
    "        next_actions = QValues.get_next_actions(policy_net, next_states)\n",
    "        next_q_values = QValues.get_next_state_q_values(target_net, next_states)\n",
    "        next_values = next_q_values.gather(dim=1, index=next_actions).flatten().detach()\n",
    "        return next_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if logger:\n",
    "    print(\"Log Directory:\", log_dir)\n",
    "\n",
    "load_model = False\n",
    "model_dir = \"runs/2048_run_20200620-191324\"\n",
    "if load_model:\n",
    "    print(\"Loading model from:\", model_dir)\n",
    "    policy_net.load_state_dict(torch.load(model_dir + \"/policy_net.model\"))\n",
    "    target_net.load_state_dict(torch.load(model_dir + \"/target_net.model\"))\n",
    "\n",
    "total_timestep = 1\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    for timestep in count():\n",
    "        total_timestep += 1\n",
    "        # print(episode, timestep, state)\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        if logger: writer.add_scalar(\"reward_per_step\", reward.item(), total_timestep)\n",
    "        if logger: writer.add_scalar(\"action\", action, total_timestep)\n",
    "        if logger: writer.add_scalar(\"exploration_rate\", strategy.exploration_rate, total_timestep)\n",
    "        # em.render('human')\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        # print(memory.push_count)\n",
    "        state = next_state\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            # Scale rewards as they can be quite large\n",
    "            # rewards = (rewards - rewards.float().mean()) / (rewards.float().std() + np.finfo(np.float32).eps)\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            # next_q_values = QValues.get_next(target_net, next_states)\n",
    "            next_q_values = QValues.get_next_double_dqn(policy_net, target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            # print(\"current_q_values\", current_q_values.flatten())\n",
    "            # print(\"target_q_values\", target_q_values.unsqueeze(1).flatten())\n",
    "            loss = nn.SmoothL1Loss()(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if logger: writer.add_scalar(\"loss\", loss, total_timestep)\n",
    "        if em.done:\n",
    "            maximum_tiles.append(em.get_max_tile())\n",
    "            if logger: writer.add_scalar(\"max_tile\", em.get_max_tile(), episode)\n",
    "            if logger: writer.add_scalar(\"episode_length\", timestep, episode)\n",
    "            if logger: writer.add_scalar(\"reward_per_ep\", reward, episode)\n",
    "            # plot(maximum_tiles, 100)\n",
    "            break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        if logger: torch.save(policy_net.state_dict(), log_dir + \"/policy_net.model\")\n",
    "        if logger: torch.save(target_net.state_dict(), log_dir + \"/target_net.model\")\n",
    "em.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Playground)",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
